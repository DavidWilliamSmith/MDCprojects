{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e00855",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a66d63",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Let's use the boston dataset again.\n",
    "\n",
    "Let's use the same DV ('medv') and one predictor ('lstat').\n",
    "\n",
    "First, let's run Simple Linear Regression and a Second Degree Poly Regression so that we can compare these two models with \n",
    "a Regression Tree.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fef99b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_data=datasets.load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288cbd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_df=pd.DataFrame(boston_data.data, columns= boston_data.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aacf5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_df['medv']= boston_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4155c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "boston_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bf8e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e471c421",
   "metadata": {},
   "source": [
    "One of the metrics that we are going to import is the mean_squared_error (MSE) because it is relevant when constructing regression trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7114d4",
   "metadata": {},
   "source": [
    "### Simple Linear Regression with Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d403a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_reg_object= LinearRegression(fit_intercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbb30bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= boston_df['LSTAT'].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d26a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y= boston_df['medv'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07446dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_reg_object.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbb9075",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round ( r2_score(y, simple_reg_object.predict(X)), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c875c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round( mean_squared_error(y, simple_reg_object.predict(X)), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f645b874",
   "metadata": {},
   "source": [
    "### Second degree polynomial regression with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd08457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly2_object= PolynomialFeatures(degree=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65416da",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly2= poly2_object.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f559a331",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly2_reg_object = LinearRegression(fit_intercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c73e2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly2_reg_object.fit(X_poly2, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81abc3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "round ( r2_score(y, poly2_reg_object.predict(X_poly2)), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e008ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adj R Squared\n",
    "\n",
    "top= sum((y - poly2_reg_object.predict(X_poly2))**2)/ (y.size-2-1)\n",
    "\n",
    "bottom= sum((y - y.mean())**2)/(y.size-1)\n",
    "\n",
    "round (1-(top/bottom), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7b5bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "round (mean_squared_error(y, poly2_reg_object.predict(X_poly2)), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765ab349",
   "metadata": {},
   "source": [
    "### Regression Tree to predict 'medv' only using 'LSTA' as predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7c7789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d089a0",
   "metadata": {},
   "source": [
    "#### Some important _hyperparameters_ of the DecisionTreeRegressor () algorithm:\n",
    "\n",
    "\n",
    "__Hyperparameters__: Parameters that you set up for the method that you are using BEFORE applying the method to a specific datatset. We have set some hyperparemters in the past, for example:\n",
    "\n",
    "- For linear regression: fit_intercept= True\n",
    "- For polynomial regression: degree= 2 (or 3, 4,..., the degree that you want to use)\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "__splitter{“best”, “random”}, default=”best”__\n",
    "\n",
    "The strategy used to choose the split at each node. Supported strategies are “best” to choose the best split and “random” to choose the best random split.\n",
    "\n",
    "<br>\n",
    "\n",
    "__max_depth: int, default=None__\n",
    "\n",
    "The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure (RSS=0) or until all leaves contain less than __min_samples_split__ samples.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "__min_samples_split: int or float, default=2__\n",
    "\n",
    "The minimum number of samples required to split an internal node:\n",
    "\n",
    "If int, then consider min_samples_split as the minimum number.\n",
    "If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.\n",
    "\n",
    "<br>\n",
    "\n",
    "__min_samples_leaf: int or float, default=1__\n",
    "\n",
    "The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least __min_samples_leaf__ training samples in each of the left and right branches. \n",
    "\n",
    "If int, then consider min_samples_leaf as the minimum number.\n",
    "If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "__max_features: int, float or {“auto”, “sqrt”, “log2”}, default=None__\n",
    "\n",
    "The number of features to consider when looking for the best split\n",
    "\n",
    "The search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than max_features features.\n",
    "\n",
    "__Comment__: For now, we will not change this feature, which means that we will have the algorithm look for all the features when attempting a split.\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "__random_state: int, RandomState instance or None, default=None__\n",
    "\n",
    "__Controls the randomness of the estimator.__\n",
    "\n",
    "When max_features < n_features, the algorithm will select max_features at random at each split before finding the best split among them__ (THAT WILL LIKELY CREATE DIFFERENT RESULTS ACROSS ATTEMPTS!!). \n",
    "\n",
    "But the best found split may vary across different runs, even if max_features=n_features__. That is the case, __if the improvement of the criterion is identical for several splits and one split has to be selected at random. \n",
    "\n",
    "__To obtain a deterministic behaviour during fitting, random_state has to be fixed to an integer.__\n",
    "\n",
    "<br>\n",
    "\n",
    "__min_impurity_decrease: float, default=0.0__\n",
    "\n",
    "min_impurity_decrease is related to the minimum decrease in RSS that we are willing to accept.\n",
    "\n",
    "A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
    "\n",
    "In regression trees, impurity = RSS (or SSE)\n",
    "\n",
    "The weighted impurity decrease equation is the following:\n",
    "\n",
    "(N_t / N) * (impurity - N_t_R / N_t * right_impurity - N_t_L / N_t * left_impurity)\n",
    "                    \n",
    "\n",
    "where,\n",
    "N is the total number of samples\n",
    "\n",
    "N_t is the number of samples at the current node\n",
    "\n",
    "N_t_L is the number of samples in the left child\n",
    "\n",
    "N_t_R is the number of samples in the right child.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0eee25",
   "metadata": {},
   "source": [
    "### Fitting a RT for medv based on lstat\n",
    "\n",
    "We will fit our first regression tree without using any refined approach for selecting the hyperparameters (we will study a more refined approach later)\n",
    "\n",
    "Let's just fit the tree defining one simple stopping criterion: the number of cases (observations) in each leave must be at least 10% of the sample size (min_samples_leaf=0.1)\n",
    "\n",
    "10% ---> 10% of 506= 50.6 ~ 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeec10d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_tree_lstat= DecisionTreeRegressor(min_samples_leaf=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1b345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_tree_lstat.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce959aea",
   "metadata": {},
   "source": [
    "__To work INDIVIDUALLY FOR A COUPLE OF MINUTES__\n",
    "\n",
    "Compute R2 for the RT by calling the method r2_score() on the RT object\n",
    "\n",
    "Compute MSE for the RT by calling the method mean_squared_error() on the RT object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19252196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R sq for RT\n",
    "\n",
    "np.round ( r2_score(y, reg_tree_lstat.predict(X)), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6e7e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE for RT\n",
    "np.round ( mean_squared_error(y, reg_tree_lstat.predict(X)), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c424f192",
   "metadata": {},
   "source": [
    "How to compute __Adjusted R Squared for a RT__? \n",
    "\n",
    "Let's try to understand the formula of Adjusted R Squared a bit better.\n",
    "\n",
    "\n",
    "Adjusted R Squared = 1 - [ (SSE/df error)/ (SSTotal/ n-1) ]\n",
    "\n",
    "a) In Linear Regression\n",
    "\n",
    "__DF error= n- (K+1), k= # predictors__\n",
    "\n",
    "Ex: For simple linear regression, K=1; therefore:\n",
    "\n",
    "DF error= n-(1+1)= n-2\n",
    "\n",
    "\n",
    "b) In general\n",
    "\n",
    "__DF error= n - # parameters to estimate__\n",
    "\n",
    "\n",
    "Ex: For a second degree polynomial equation on X:\n",
    "\n",
    "DF error= n - 3 (3 parameters to estimate: the intercept, the coeff for X, and the coeff for X squared)\n",
    "\n",
    "\n",
    "Ex: In a Regression Tree\n",
    "\n",
    "DF error= n- (# of regions)\n",
    "\n",
    "For each region, the RT method needs to estimate the mean of Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15e3c04",
   "metadata": {},
   "source": [
    "Computing adjusted R squared for the regression tree\n",
    "\n",
    "We need to know how many regions resulted from the tree. How to find out the number of regions?\n",
    "\n",
    "Option 1: Plot the tree and observe the number of regions (= number of leaves)\n",
    "\n",
    "Option 2: Count the number of leaf nodes (see next code cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac6f08d",
   "metadata": {},
   "source": [
    "When we call the methods '.tree_.children_left' or '.tree_.children_right' on the tree object, we get an array with the indexes of the nodes to the left and right of each node. \n",
    "\n",
    "These methods return a -1 when the node to the left or right of a given node is a leaf node.\n",
    "\n",
    "Therefore, we can know the number of leaves in the tree by counting the number of -1 in the ouput of '.tree_.children_left' or '.tree_.children_right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3c6a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_tree_lstat.tree_.children_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2283419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_tree_lstat.tree_.children_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c328127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of leaves in the tree by counting the number of -1 in any of the above arrays\n",
    "# Save it in a variable called 'number_leaves'\n",
    "# DO IT HERE\n",
    "\n",
    "number_leaves = np.sum(reg_tree_lstat.tree_.children_right==-1)\n",
    "number_leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875316f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a direct method to get the number of leaves\n",
    "\n",
    "reg_tree_lstat.get_n_leaves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64c570c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted R squared fo the tree\n",
    "\n",
    "top= sum((y - reg_tree_lstat.predict(X))**2)/ (y.size-number_leaves)\n",
    "\n",
    "bottom= (sum((y - y.mean())**2))/(y.size-1)\n",
    "\n",
    "round (1-(top/bottom), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04624c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot the tree!\n",
    "\n",
    "tree.plot_tree(reg_tree_lstat)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348d042c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The figsize parameter sets the plot size (in inches)\n",
    "\n",
    "plt.figure(figsize=(12,12))   \n",
    "tree.plot_tree(reg_tree_lstat, fontsize=10, feature_names=['LSTAT'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66bb5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MSE of the top node is the variance of medv (= the variance of Y)\n",
    "\n",
    "np.round( np.var(boston_df['medv']), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80fee30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The prediction of Y for the top node is just the mean of Y\n",
    "\n",
    "np.round( np.mean(boston_df['medv']), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5509ca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MSE of the left node on the second layer is the variance of medv for the observations where the LSTAT <= 9.725\n",
    "\n",
    "np.round(np.var (boston_df['medv'][boston_df['LSTAT']<=9.725]), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76503437",
   "metadata": {},
   "source": [
    "### Interpret the tree\n",
    "##### Take 3 mins and try to answer the folowing questions independently. We will discuss the answers in 3 mins\n",
    "\n",
    "What is the max depth of the tree?\n",
    "\n",
    "How many regions were created by the tree algorithm?\n",
    "\n",
    "Use the tree and graphically (i.e., going down the tree branches) predict the __medv__ for a neighborhood where lstat is 15.\n",
    "\n",
    "Does the tree evidence any relationship (positive or negative) between lstat and medv? Where do you see that relationship in the tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec3aa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a method to get the depth of the tree\n",
    "\n",
    "reg_tree_lstat.get_depth()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc01c09",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Let's plot the lines for the three models (simple reg, poly reg, and the tree) to see if we can graphically observe the reason why the RT does better than the linear regression and seemingly even better than the poly of second degree!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c5abf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')\n",
    "\n",
    "plt.scatter(boston_df['LSTAT'], boston_df['medv'],c='grey',marker='o')\n",
    "\n",
    "plt.xlabel(\"Lstat\")\n",
    "\n",
    "plt.ylabel(\"Medv\")\n",
    "\n",
    "\n",
    "plt.plot(boston_df['LSTAT'], simple_reg_object.predict(X), c='red', ls='-', label='Linear Model')\n",
    "\n",
    "\n",
    "plt.plot(boston_df['LSTAT'].sort_values(), reg_tree_lstat.predict(boston_df['LSTAT'].sort_values().values.reshape(-1,1)), c='black', ls='-', label='Reg Tree')\n",
    "\n",
    "\n",
    "plt.plot(boston_df['LSTAT'].sort_values(), poly2_reg_object.fit(X_poly2, y).predict(poly2_object.fit_transform(boston_df['LSTAT'].sort_values().values.reshape(-1,1))), c='green', ls='-', label='Poly 2nd degree')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# plt.legend() will show a legend that correctly identifies each model because \n",
    "# the parameter 'label' was passed to the plot of each model\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4284d0",
   "metadata": {},
   "source": [
    "It is clear from the scatteplot that both the tree and the second-degree poly fit the data better than the linear equation.\n",
    "\n",
    "However, I am not fully convinced that the tree does better than the second-degree poly as the adjusted R squared suggested.\n",
    "\n",
    "Why don't we estimate the test prediction error for both the tree and the second-degree poly and compare them?\n",
    "\n",
    "Let's do this using CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6222393b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578413e6",
   "metadata": {},
   "source": [
    "Poly of second degree. Getting the CV error for each of the ten iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad96bae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_values=-1*cross_val_score(poly2_reg_object.fit(X_poly2, y), X_poly2, y, scoring= 'neg_mean_squared_error', cv=10)\n",
    "cv_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a058ce",
   "metadata": {},
   "source": [
    "Mean CV error = Mean test MSE ~ Estimation of test prediction error for Poly of second degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef79fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(cv_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a55ef6c",
   "metadata": {},
   "source": [
    "Regression Tree. Getting the CV error for each of the ten iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c16342",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_values2=-1*cross_val_score(reg_tree_lstat.fit(X, y), X, y, scoring= 'neg_mean_squared_error', cv=10)\n",
    "cv_values2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11fa980",
   "metadata": {},
   "source": [
    "Mean CV error = Mean test MSE ~ Estimation of test prediction error for Reg Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a232610",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(cv_values2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c02d46",
   "metadata": {},
   "source": [
    "### A better CV implementation that shuffles the data before splitting it into K folds\n",
    "\n",
    "### REVIEW INDEPENDENTLY AT HOME!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b3cecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e873d621",
   "metadata": {},
   "outputs": [],
   "source": [
    "k10fold=KFold(n_splits=10, shuffle=True, random_state= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3946c12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes= np.arange(len(boston_df['medv']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894850d6",
   "metadata": {},
   "source": [
    "CV application for the second degree poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30debf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores=np.empty(shape=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c52eb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for train_index, test_index in k10fold.split(indexes):\n",
    "    regression_model=smf.ols('medv~LSTAT+I(LSTAT**2)', data=boston_df.iloc[train_index,]).fit()\n",
    "    predictions=regression_model.predict(boston_df['LSTAT'][test_index])\n",
    "    # The next line computes the test Mean Squared Error for each iteration\n",
    "    cv_scores[i]=sum((boston_df['medv'][test_index] -predictions)**2)/(test_index.size)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaba690",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean (cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e8f09f",
   "metadata": {},
   "source": [
    "CV application for the Regression Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aa6726",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores2=np.empty(shape=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f048aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for train_index, test_index in k10fold.split(indexes):\n",
    "    regression_model=reg_tree_lstat.fit(X[train_index], y[train_index])\n",
    "    predictions=regression_model.predict(X[test_index])\n",
    "    # The next line computes the test Mean Squared Error for each iteration\n",
    "    cv_scores2[i]=sum((y[test_index] -predictions)**2)/(test_index.size)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c2801c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean (cv_scores2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b65576e",
   "metadata": {},
   "source": [
    "### START HERE ON TUESDAY 10-04 !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f748fd6d",
   "metadata": {},
   "source": [
    "Share the methods I found with students:\n",
    "\n",
    "get_n_leaves()\n",
    "\n",
    "get_depth()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cf2b2e",
   "metadata": {},
   "source": [
    "### Regression Tree to predict 'medv' using all the predictors in the Boston dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6668e06",
   "metadata": {},
   "source": [
    "Now we are going to learn how to apply RT in a more refined way:\n",
    "\n",
    "a) Considering multiple predictors\n",
    "\n",
    "b) Following a __conventional selection methodology__ used to estimate a good regression tree. This selection methodology applies a __pre-pruning strategy__ to growth the tree.\n",
    "\n",
    "__Pre-pruning__: One more or more stopping criteria applied to the tree to prevent it from learning the training set without error. \n",
    "\n",
    "__Important__: In ML, when you hear that someone is prunning a tree that usually refers to the application of a __post-pruning strategy__, a topic we will cover on the next session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f96c990",
   "metadata": {},
   "source": [
    "What is the __conventional selection methodology__ used to obtain a good regression tree?\n",
    "\n",
    "Check out the scikit-learn documentation for a graphical representation of this:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation\n",
    "\n",
    "\n",
    "1) The data is split in training data and test data (for ex, 80% training and 20% test). \n",
    "\n",
    "The training data is used AS IF IT WAS ALL THE AVAILABLE DATA and CV is applied on it. CV is applied with the purpose of tuning the hyperparameters of the algorithm and find a good Tree. \n",
    "\n",
    "By tuning the hyperparameters of the algorithm I mean finding a good combination of hyperparameters (i.e., choosing max_depth, min_sample_splits, min_impurity_decrease, etc).\n",
    "\n",
    "\n",
    "2) The good combination of hyperparameters found after applying CV are used to estimate a Tree using the training data.\n",
    "\n",
    "3) Finally, the performance of this Tree is evaluated on the testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54e4b56",
   "metadata": {},
   "source": [
    "1) The data is split in training data and test data (next cells):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2304b513",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bca528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test= train_test_split (boston_df.iloc[:,:-1],boston_df['medv'], test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e339eb",
   "metadata": {},
   "source": [
    "Now, we tune the hyperparameters of the regression tree algorithm by using CV\n",
    "\n",
    "This is an INTENSE application of CV because __we need to apply CV for each combination of hyperparametes__... and there are many combinations of hyperparameters.\n",
    "\n",
    "There is a class from scikit-learn called 'GridSearchCV' that facilitates the application of CV in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafa2ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542ae064",
   "metadata": {},
   "source": [
    "The first step before applying the GridSearchCV() method is to create a dictionary with a range of values for each parameter (next cell):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e803d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_grid = {\n",
    "    'max_depth': np.arange(2,11), # testing depth from 2 to 10\n",
    "    'min_samples_split':[0.1, 0.15, 0.2],\n",
    "    'min_samples_leaf':[0.05, 0.1, 0.15], \n",
    "    'min_impurity_decrease': [0, 0.0005, 0.001, 0.005, 0.01, 0.05]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c7868a",
   "metadata": {},
   "source": [
    "One recommendation (just a recommendation !!!) is to apply GridSearchCV() two times: a first time to narrow down the possible good values of the hyperparameters and the second time to choose the final values of the hyperparameters to use in building the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc6cb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridSearch1 = GridSearchCV(DecisionTreeRegressor(), hyperparam_grid, cv=5, scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae45b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridSearch1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f0af5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Initial parameters: ', gridSearch1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa091998",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_grid2 = {\n",
    "    'max_depth': [3,4,5,6,7],  \n",
    "     'min_samples_split': [0.08, 0.1, 0.12], \n",
    "    'min_samples_leaf': [0.02, 0.05, 0.08],   \n",
    "    'min_impurity_decrease': [0.001, 0.005, 0.01]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e20c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridSearch2 = GridSearchCV(DecisionTreeRegressor(), hyperparam_grid2, cv=5,scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b282befc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridSearch2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff007f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Improved parameters: ', gridSearch2.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e298dc",
   "metadata": {},
   "source": [
    "Now, we need to build the tree with these hyperparameters values (or a combination of these values and those we found in the previous step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7166f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_tree_multiple_boston= DecisionTreeRegressor(max_depth= 7, min_samples_split= 0.08, min_samples_leaf= 0.02, min_impurity_decrease= 0.001, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5de191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the tree on the training data using the previous hyperparameters\n",
    "\n",
    "reg_tree_multiple_boston.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c73175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we evaluate the prediction performance of 'reg_tree_multiple_boston' on the test data\n",
    "\n",
    "mean_squared_error( y_test, reg_tree_multiple_boston.predict (X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfb21cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the tree\n",
    "\n",
    "plt.figure(figsize=(25,20))   \n",
    "tree.plot_tree(reg_tree_multiple_boston,filled=True, rounded= True, feature_names=X_train.columns, fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d8fe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What predictors were selected to be in the tree?\n",
    "\n",
    "boston_df.iloc[:,:-1].columns [reg_tree_multiple_boston.feature_importances_!=0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b161b6",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "__Finally__, just check to see if the hyperparameters are actually being used when building the tree:\n",
    "\n",
    "Max_depth was 7. Is this true in the tree?\n",
    "\n",
    "min_samples_leaf was 2%. Is this true in the tree?\n",
    "\n",
    "THINK ABOUT THIS FOR 2 MINUTES !!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
